#!/bin/sh
# This script downloads/syncs given URLs to disk.
#
# Requirements: Curl, Coreutils
# Usage: As first command-line parameter you can give a location to 
#        a feed store directory. Via stdin you must give one URL per
#        line. The path of successful downloads will be printed to stdout.
#        Errors are printed to stderr. Exit code 1 on inaccessible store,
#        otherwise 0, even with other (non-fatal) errors.

[ $# -ne 1 ] && store=/tmp/mariurss-store/ || store=$1
[ ! -d "$store" ] && { mkdir -p "$store" 2>/dev/null || { echo "Can't access/create store directory at '$store'" >&2; exit 1; }; }
cd "$store"

fetch() {
    url=$1
    ID=$(echo "$url" | sha1sum | head -c 40)
    file=$ID

    # Skip if line is empty or a comment (that is: it starts with hashtag)
    [ -z "$url" ] || [ "${url#\#}" != "${url}" ] && return

    # We aggressively track errors here because tracking them later in the 
    # toolchain is a pain in the butt. To ensure all error messages are 
    # printed one after another and not in parallel we print it in one 
    # single echo statement.
    ERRFILE="$(mktemp)"
    if curl --silent --show-error --fail \
        --location \
        --connect-timeout 60 \
        --output "$file" \
        --tcp-fastopen \
        --time-cond "$file" \
        "$url" > "$ERRFILE" 2>&1
    then
        if [ -e "$file" ]; then
	    # Print filename
	    echo "$(pwd)/$file"

	    if [ "$(cat "$(pwd)/$file" | wc -c)" = "0" ]; then
                echo "Warning: Got empty response for URL '$url'" >&2
	    fi
        fi    
    else
        echo "Error: Could not download file." "\n\tURL: '$url'" "\n\t$(cat "$ERRFILE")" >&2
    fi
}

while IFS=$'\n' read -r url; do
    # Space removal advantages: (1) turning comments (using '#')
    # into valid URLs, (2) allows passing the URL as single variable
    # to fetch()
    fetch $(echo "$url" | sed "s/ //g") &
done

wait # wait for async fetches to finish
